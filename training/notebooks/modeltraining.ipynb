{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nf9pqHdIuuOh",
        "outputId": "34770db7-b923-437f-e648-a6bbb85b4bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'colab'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 52 (delta 12), reused 49 (delta 9), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (52/52), 15.54 KiB | 2.59 MiB/s, done.\n",
            "Resolving deltas: 100% (12/12), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AdamFehse/colab.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FX497qMEv7BT"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch numpy pandas requests tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQyIbcHcxXlQ",
        "outputId": "bf573cad-c8eb-4578-d350-acf226831144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Available: False\n",
            "GPU Name: N/A\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eJUil4Bxf8p",
        "outputId": "49b48855-fa60-4b73-9cee-d1e0d64bee44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/colab/training\n"
          ]
        }
      ],
      "source": [
        "%cd training/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04cd886a"
      },
      "source": [
        "Let's run this code to fetch the valid column names from the NASA Exoplanet Archive for the `ps` table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c19f6811",
        "outputId": "96da08c6-80ac-40e9-cc40-d76681b9e581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Starting exoplanet data preprocessing...\n",
            "INFO:__main__:Fetching NASA exoplanet data...\n",
            "INFO:__main__:Downloaded 1069 exoplanet records\n",
            "INFO:__main__:Filtering for multi-planet systems...\n",
            "INFO:__main__:Filtered to 232 multi-planet systems\n",
            "INFO:__main__:Normalizing parameters...\n",
            "INFO:__main__:Parameters normalized and stats saved\n",
            "INFO:__main__:Creating transformer sequences...\n",
            "INFO:__main__:Created 232 sequences\n",
            "INFO:__main__:Augmenting data 3x...\n",
            "INFO:__main__:Augmented to 696 total sequences\n",
            "INFO:__main__:Saved 696 augmented sequences to training/data/processed/train.npz\n",
            "INFO:__main__:Dataset shape: (696, 64)\n",
            "✓ Preprocessing complete: (696, 64)\n",
            "✓ Sample sequence (first 10 tokens): [  0  76  10 184  76   3  66  71 154  99]\n"
          ]
        }
      ],
      "source": [
        "!python data/preprocessing.py --profile test2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e66dc29"
      },
      "source": [
        "After preprocessing is complete, we'll run the training script with the `--profile test2` flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b628fc9",
        "outputId": "352843e9-0b73-45a0-ca7e-7b208998df04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:__main__:=== Transformer Training ===\n",
            "INFO:__main__:Data: training/data/processed/train.npz\n",
            "INFO:__main__:Device: cuda\n",
            "INFO:__main__:Epochs: 8\n",
            "INFO:__main__:Batch size: 16\n",
            "INFO:__main__:Learning rate: 0.0001\n",
            "INFO:__main__:Profile: test2\n",
            "INFO:__main__:Max samples: 8000\n",
            "INFO:__main__:\n",
            "Loading data...\n",
            "INFO:__main__:Loaded 696 sequences\n",
            "INFO:__main__:Sequence shape: (696, 64)\n",
            "INFO:__main__:Train: 626, Val: 70\n",
            "INFO:__main__:\n",
            "Creating model...\n",
            "INFO:__main__:Model: 1,255,680 parameters\n",
            "INFO:__main__:\n",
            "Training...\n",
            "INFO:__main__:\n",
            "Epoch 1/8\n",
            "INFO:__main__:  Batch 10/40: loss=4.9796\n",
            "INFO:__main__:  Batch 20/40: loss=4.5587\n",
            "INFO:__main__:  Batch 30/40: loss=4.4285\n",
            "INFO:__main__:  Batch 40/40: loss=4.1657\n",
            "INFO:__main__:Train loss: 4.5995\n",
            "INFO:__main__:Val loss: 3.8739\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=3.8739)\n",
            "INFO:__main__:\n",
            "Epoch 2/8\n",
            "INFO:__main__:  Batch 10/40: loss=3.9998\n",
            "INFO:__main__:  Batch 20/40: loss=3.8046\n",
            "INFO:__main__:  Batch 30/40: loss=4.0451\n",
            "INFO:__main__:  Batch 40/40: loss=3.8466\n",
            "INFO:__main__:Train loss: 4.0301\n",
            "INFO:__main__:Val loss: 3.5876\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=3.5876)\n",
            "INFO:__main__:\n",
            "Epoch 3/8\n",
            "INFO:__main__:  Batch 10/40: loss=3.6783\n",
            "INFO:__main__:  Batch 20/40: loss=4.0816\n",
            "INFO:__main__:  Batch 30/40: loss=3.5881\n",
            "INFO:__main__:  Batch 40/40: loss=3.3464\n",
            "INFO:__main__:Train loss: 3.7979\n",
            "INFO:__main__:Val loss: 3.4229\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=3.4229)\n",
            "INFO:__main__:\n",
            "Epoch 4/8\n",
            "INFO:__main__:  Batch 10/40: loss=3.4334\n",
            "INFO:__main__:  Batch 20/40: loss=3.6758\n",
            "INFO:__main__:  Batch 30/40: loss=3.9939\n",
            "INFO:__main__:  Batch 40/40: loss=3.6637\n",
            "INFO:__main__:Train loss: 3.6635\n",
            "INFO:__main__:Val loss: 3.3221\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=3.3221)\n",
            "INFO:__main__:\n",
            "Epoch 5/8\n",
            "INFO:__main__:  Batch 10/40: loss=3.7917\n",
            "INFO:__main__:  Batch 20/40: loss=3.4680\n",
            "INFO:__main__:  Batch 30/40: loss=3.4346\n",
            "INFO:__main__:  Batch 40/40: loss=3.1881\n",
            "INFO:__main__:Train loss: 3.5681\n",
            "INFO:__main__:Val loss: 3.2599\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=3.2599)\n",
            "INFO:__main__:\n",
            "Epoch 6/8\n",
            "INFO:__main__:  Batch 10/40: loss=3.4164\n",
            "INFO:__main__:  Batch 20/40: loss=3.5529\n",
            "INFO:__main__:  Batch 30/40: loss=3.7368\n",
            "INFO:__main__:  Batch 40/40: loss=3.4672\n",
            "INFO:__main__:Train loss: 3.5233\n",
            "INFO:__main__:Val loss: 3.2292\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=3.2292)\n",
            "INFO:__main__:\n",
            "Epoch 7/8\n",
            "INFO:__main__:  Batch 10/40: loss=3.4756\n",
            "INFO:__main__:  Batch 20/40: loss=3.3750\n",
            "INFO:__main__:  Batch 30/40: loss=3.4695\n",
            "INFO:__main__:  Batch 40/40: loss=3.6996\n",
            "INFO:__main__:Train loss: 3.4906\n",
            "INFO:__main__:Val loss: 3.2151\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=3.2151)\n",
            "INFO:__main__:\n",
            "Epoch 8/8\n",
            "INFO:__main__:  Batch 10/40: loss=3.6339\n",
            "INFO:__main__:  Batch 20/40: loss=3.1553\n",
            "INFO:__main__:  Batch 30/40: loss=3.3647\n",
            "INFO:__main__:  Batch 40/40: loss=3.5414\n",
            "INFO:__main__:Train loss: 3.4779\n",
            "INFO:__main__:Val loss: 3.2112\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=3.2112)\n",
            "INFO:__main__:\n",
            "✓ Training complete\n",
            "INFO:__main__:Best model saved to: ../models/transformer_v1.pt\n"
          ]
        }
      ],
      "source": [
        "!python train.py --data training/data/processed/train.npz --output ../models/transformer_v1.pt --profile test2 --device cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1939b722",
        "outputId": "874a234e-536d-4214-c06b-30ab1601a845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "normalization_stats.json\n",
            "train.npz\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "ls /content/colab/training/training/data/processed/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21d788a6",
        "outputId": "d0005ad0-e616-4bfc-f5b7-aafcb10b83ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['pl_name', 'pl_letter', 'hostname', 'hd_name', 'hip_name', 'tic_id', 'default_flag', 'pl_refname', 'st_refname', 'sy_refname', 'disc_pubdate', 'disc_year', 'discoverymethod', 'disc_locale', 'disc_facility', 'disc_instrument', 'disc_telescope', 'disc_refname', 'ra', 'rastr', 'dec', 'decstr', 'glon', 'glat', 'elon', 'elat', 'pl_orbper', 'pl_orbpererr1', 'pl_orbpererr2', 'pl_orbperlim', 'pl_orbperstr', 'pl_orblpererr1', 'pl_orblper', 'pl_orblpererr2', 'pl_orblperlim', 'pl_orblperstr', 'pl_orbsmax', 'pl_orbsmaxerr1', 'pl_orbsmaxerr2', 'pl_orbsmaxlim', 'pl_orbsmaxstr', 'pl_orbincl', 'pl_orbinclerr1', 'pl_orbinclerr2', 'pl_orbincllim', 'pl_orbinclstr', 'pl_orbtper', 'pl_orbtpererr1', 'pl_orbtpererr2', 'pl_orbtperlim', 'pl_orbtperstr', 'pl_orbeccen', 'pl_orbeccenerr1', 'pl_orbeccenerr2', 'pl_orbeccenlim', 'pl_orbeccenstr', 'pl_eqt', 'pl_eqterr1', 'pl_eqterr2', 'pl_eqtlim', 'pl_eqtstr', 'pl_occdep', 'pl_occdeperr1', 'pl_occdeperr2', 'pl_occdeplim', 'pl_occdepstr', 'pl_insol', 'pl_insolerr1', 'pl_insolerr2', 'pl_insollim', 'pl_insolstr', 'pl_dens', 'pl_denserr1', 'pl_denserr2', 'pl_denslim', 'pl_densstr', 'pl_trandep', 'pl_trandeperr1', 'pl_trandeperr2', 'pl_trandeplim', 'pl_trandepstr', 'pl_tranmid', 'pl_tranmiderr1', 'pl_tranmiderr2', 'pl_tranmidlim', 'pl_tranmidstr', 'pl_trandur', 'pl_trandurerr1', 'pl_trandurerr2', 'pl_trandurlim', 'pl_trandurstr', 'pl_rvamp', 'pl_rvamperr1', 'pl_rvamperr2', 'pl_rvamplim', 'pl_rvampstr', 'pl_radj', 'pl_radjerr1', 'pl_radjerr2', 'pl_radjlim', 'pl_radjstr', 'pl_rade', 'pl_radeerr1', 'pl_radeerr2', 'pl_radelim', 'pl_radestr', 'pl_ratror', 'pl_ratrorerr1', 'pl_ratrorerr2', 'pl_ratrorlim', 'pl_ratrorstr', 'pl_ratdor', 'pl_ratdorerr1', 'pl_ratdorerr2', 'pl_ratdorlim', 'pl_ratdorstr', 'pl_imppar', 'pl_impparerr1', 'pl_impparerr2', 'pl_impparlim', 'pl_impparstr', 'pl_cmassj', 'pl_cmassjerr1', 'pl_cmassjerr2', 'pl_cmassjlim', 'pl_cmassjstr', 'pl_cmasse', 'pl_cmasseerr1', 'pl_cmasseerr2', 'pl_cmasselim', 'pl_cmassestr', 'pl_massj', 'pl_massjerr1', 'pl_massjerr2', 'pl_massjlim', 'pl_massjstr', 'pl_masse', 'pl_masseerr1', 'pl_masseerr2', 'pl_masselim', 'pl_massestr', 'pl_bmassj', 'pl_bmassjerr1', 'pl_bmassjerr2', 'pl_bmassjlim', 'pl_bmassjstr', 'pl_bmasse', 'pl_bmasseerr1', 'pl_bmasseerr2', 'pl_bmasselim', 'pl_bmassestr', 'pl_bmassprov', 'pl_msinij', 'pl_msinijerr1', 'pl_msinijerr2', 'pl_msinijlim', 'pl_msinijstr', 'pl_msinie', 'pl_msinieerr1', 'pl_msinieerr2', 'pl_msinielim', 'pl_msiniestr', 'st_teff', 'st_tefferr1', 'st_tefferr2', 'st_tefflim', 'st_teffstr', 'st_met', 'st_meterr1', 'st_meterr2', 'st_metlim', 'st_metstr', 'st_radv', 'st_radverr1', 'st_radverr2', 'st_radvlim', 'st_radvstr', 'st_vsin', 'st_vsinerr1', 'st_vsinerr2', 'st_vsinlim', 'st_vsinstr', 'st_lum', 'st_lumerr1', 'st_lumerr2', 'st_lumlim', 'st_lumstr', 'st_logg', 'st_loggerr1', 'st_loggerr2', 'st_logglim', 'st_loggstr', 'st_age', 'st_ageerr1', 'st_ageerr2', 'st_agelim', 'st_agestr', 'st_mass', 'st_masserr1', 'st_masserr2', 'st_masslim', 'st_massstr', 'st_dens', 'st_denserr1', 'st_denserr2', 'st_denslim', 'st_densstr', 'st_rad', 'st_raderr1', 'st_raderr2', 'st_radlim', 'st_radstr', 'ttv_flag', 'ptv_flag', 'tran_flag', 'rv_flag', 'ast_flag', 'obm_flag', 'micro_flag', 'etv_flag', 'ima_flag', 'pul_flag', 'soltype', 'sy_snum', 'sy_pnum', 'sy_mnum', 'st_nphot', 'st_nrvc', 'st_nspec', 'pl_nnotes', 'pl_ntranspec', 'pl_nespec', 'pl_ndispec', 'sy_pm', 'sy_pmerr1', 'sy_pmerr2', 'sy_pmstr', 'sy_pmra', 'sy_pmraerr1', 'sy_pmraerr2', 'sy_pmrastr', 'sy_pmdec', 'sy_pmdecerr1', 'sy_pmdecerr2', 'sy_pmdecstr', 'sy_plx', 'sy_plxerr1', 'sy_plxerr2', 'sy_plxstr', 'sy_dist', 'sy_disterr1', 'sy_disterr2', 'sy_diststr', 'sy_bmag', 'sy_bmagerr1', 'sy_bmagerr2', 'sy_bmagstr', 'sy_vmag', 'sy_vmagerr1', 'sy_vmagerr2', 'sy_vmagstr', 'sy_jmag', 'sy_jmagerr1', 'sy_jmagerr2', 'sy_jmagstr', 'sy_hmag', 'sy_hmagerr1', 'sy_hmagerr2', 'sy_hmagstr', 'sy_kmag', 'sy_kmagerr1', 'sy_kmagerr2', 'sy_kmagstr', 'sy_umag', 'sy_umagerr1', 'sy_umagerr2', 'sy_umagstr', 'sy_rmag', 'sy_rmagerr1', 'sy_rmagerr2', 'sy_rmagstr', 'sy_imag', 'sy_imagerr1', 'sy_imagerr2', 'sy_imagstr', 'sy_zmag', 'sy_zmagerr1', 'sy_zmagerr2', 'sy_zmagstr', 'sy_w1mag', 'sy_w1magerr1', 'sy_w1magerr2', 'sy_w1magstr', 'sy_w2mag', 'sy_w2magerr1', 'sy_w2magerr2', 'sy_w2magstr', 'sy_w3mag', 'sy_w3magerr1', 'sy_w3magerr2', 'sy_w3magstr', 'sy_w4mag', 'sy_w4magerr1', 'sy_w4magerr2', 'sy_w4magstr', 'sy_gmag', 'sy_gmagerr1', 'sy_gmagerr2', 'sy_gmagstr', 'sy_gaiamag', 'sy_gaiamagerr1', 'sy_gaiamagerr2', 'sy_gaiamagstr', 'sy_tmag', 'sy_tmagerr1', 'sy_tmagerr2', 'sy_tmagstr', 'pl_controv_flag', 'pl_tsystemref', 'st_metratio', 'st_spectype', 'sy_kepmag', 'sy_kepmagerr1', 'sy_kepmagerr2', 'sy_kepmagstr', 'st_rotp', 'st_rotperr1', 'st_rotperr2', 'st_rotplim', 'st_rotpstr', 'pl_projobliq', 'pl_projobliqerr1', 'pl_projobliqerr2', 'pl_projobliqlim', 'pl_projobliqstr', 'x', 'pl_trueobliq', 'pl_trueobliqerr1', 'y', 'z', 'pl_trueobliqerr2', 'htm20', 'pl_trueobliqlim', 'cb_flag', 'pl_trueobliqstr', 'sy_icmag', 'sy_icmagerr1', 'sy_icmagerr2', 'sy_icmagstr', 'rowupdate', 'pl_pubdate', 'releasedate', 'dkin_flag', 'gaia_dr2_id', 'gaia_dr3_id']\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync\"\n",
        "schema_query = \"SELECT column_name FROM TAP_SCHEMA.columns WHERE table_name=\\'ps\\'\"\n",
        "resp = requests.get(url, params={\"query\": schema_query, \"format\": \"csv\"})\n",
        "resp.raise_for_status()\n",
        "cols = pd.read_csv(StringIO(resp.text))\n",
        "print(cols['column_name'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab3fdb76",
        "outputId": "5d7ce9ec-87c0-4eee-dec9-b9747d9ce21e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Starting exoplanet data preprocessing...\n",
            "INFO:__main__:Fetching NASA exoplanet data...\n",
            "INFO:__main__:Downloaded 1069 exoplanet records\n",
            "INFO:__main__:Filtering for multi-planet systems...\n",
            "INFO:__main__:Filtered to 232 multi-planet systems\n",
            "INFO:__main__:Normalizing parameters...\n",
            "INFO:__main__:Parameters normalized and stats saved\n",
            "INFO:__main__:Creating transformer sequences...\n",
            "INFO:__main__:Created 232 sequences\n",
            "INFO:__main__:Augmenting data 10x...\n",
            "INFO:__main__:Augmented to 2320 total sequences\n",
            "INFO:__main__:Saved 2320 augmented sequences to training/data/processed/train.npz\n",
            "INFO:__main__:Dataset shape: (2320, 64)\n",
            "✓ Preprocessing complete: (2320, 64)\n",
            "✓ Sample sequence (first 10 tokens): [  0  76  10 184  76   3  66  71 154  99]\n"
          ]
        }
      ],
      "source": [
        "!python data/preprocessing.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5R5kMZhTmCpY",
        "outputId": "57d689b2-7231-4b3c-c278-9f9dbab337db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:__main__:=== Transformer Training ===\n",
            "INFO:__main__:Data: training/data/processed/train.npz\n",
            "INFO:__main__:Device: cuda\n",
            "INFO:__main__:Epochs: 50\n",
            "INFO:__main__:Batch size: 32\n",
            "INFO:__main__:Learning rate: 0.0001\n",
            "INFO:__main__:\n",
            "Loading data...\n",
            "INFO:__main__:Loaded 2320 sequences\n",
            "INFO:__main__:Sequence shape: (2320, 64)\n",
            "INFO:__main__:Train: 2088, Val: 232\n",
            "INFO:__main__:\n",
            "Creating model...\n",
            "INFO:__main__:Model: 1,255,680 parameters\n",
            "INFO:__main__:\n",
            "Training...\n",
            "INFO:__main__:\n",
            "Epoch 1/50\n",
            "INFO:__main__:  Batch 10/66: loss=4.7487\n",
            "INFO:__main__:  Batch 20/66: loss=4.1759\n",
            "INFO:__main__:  Batch 30/66: loss=4.1853\n",
            "INFO:__main__:  Batch 40/66: loss=3.8863\n",
            "INFO:__main__:  Batch 50/66: loss=3.8659\n",
            "INFO:__main__:  Batch 60/66: loss=3.8401\n",
            "INFO:__main__:Train loss: 4.1542\n",
            "INFO:__main__:Val loss: 3.5861\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=3.5861)\n",
            "INFO:__main__:\n",
            "Epoch 2/50\n",
            "INFO:__main__:  Batch 10/66: loss=3.6335\n",
            "INFO:__main__:  Batch 20/66: loss=3.7164\n",
            "INFO:__main__:  Batch 30/66: loss=3.8627\n",
            "INFO:__main__:  Batch 40/66: loss=3.3152\n",
            "INFO:__main__:  Batch 50/66: loss=3.4569\n",
            "INFO:__main__:  Batch 60/66: loss=3.4139\n",
            "INFO:__main__:Train loss: 3.5045\n",
            "INFO:__main__:Val loss: 3.2430\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=3.2430)\n",
            "INFO:__main__:\n",
            "Epoch 3/50\n",
            "INFO:__main__:  Batch 10/66: loss=3.3125\n",
            "INFO:__main__:  Batch 20/66: loss=3.2962\n",
            "INFO:__main__:  Batch 30/66: loss=3.3814\n",
            "INFO:__main__:  Batch 40/66: loss=3.1450\n",
            "INFO:__main__:  Batch 50/66: loss=3.1451\n",
            "INFO:__main__:  Batch 60/66: loss=3.2856\n",
            "INFO:__main__:Train loss: 3.2471\n",
            "INFO:__main__:Val loss: 3.0558\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=3.0558)\n",
            "INFO:__main__:\n",
            "Epoch 4/50\n",
            "INFO:__main__:  Batch 10/66: loss=3.0561\n",
            "INFO:__main__:  Batch 20/66: loss=3.0962\n",
            "INFO:__main__:  Batch 30/66: loss=3.3499\n",
            "INFO:__main__:  Batch 40/66: loss=3.0452\n",
            "INFO:__main__:  Batch 50/66: loss=2.9144\n",
            "INFO:__main__:  Batch 60/66: loss=3.1003\n",
            "INFO:__main__:Train loss: 3.0790\n",
            "INFO:__main__:Val loss: 2.9165\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.9165)\n",
            "INFO:__main__:\n",
            "Epoch 5/50\n",
            "INFO:__main__:  Batch 10/66: loss=3.0307\n",
            "INFO:__main__:  Batch 20/66: loss=3.1675\n",
            "INFO:__main__:  Batch 30/66: loss=2.9672\n",
            "INFO:__main__:  Batch 40/66: loss=2.8010\n",
            "INFO:__main__:  Batch 50/66: loss=2.8966\n",
            "INFO:__main__:  Batch 60/66: loss=2.9098\n",
            "INFO:__main__:Train loss: 2.9518\n",
            "INFO:__main__:Val loss: 2.8054\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.8054)\n",
            "INFO:__main__:\n",
            "Epoch 6/50\n",
            "INFO:__main__:  Batch 10/66: loss=3.0173\n",
            "INFO:__main__:  Batch 20/66: loss=3.0166\n",
            "INFO:__main__:  Batch 30/66: loss=2.8291\n",
            "INFO:__main__:  Batch 40/66: loss=2.9448\n",
            "INFO:__main__:  Batch 50/66: loss=2.8484\n",
            "INFO:__main__:  Batch 60/66: loss=2.9035\n",
            "INFO:__main__:Train loss: 2.8408\n",
            "INFO:__main__:Val loss: 2.7134\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.7134)\n",
            "INFO:__main__:\n",
            "Epoch 7/50\n",
            "INFO:__main__:  Batch 10/66: loss=2.6710\n",
            "INFO:__main__:  Batch 20/66: loss=2.7821\n",
            "INFO:__main__:  Batch 30/66: loss=2.8959\n",
            "INFO:__main__:  Batch 40/66: loss=2.7027\n",
            "INFO:__main__:  Batch 50/66: loss=2.6824\n",
            "INFO:__main__:  Batch 60/66: loss=2.8135\n",
            "INFO:__main__:Train loss: 2.7500\n",
            "INFO:__main__:Val loss: 2.6307\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.6307)\n",
            "INFO:__main__:\n",
            "Epoch 8/50\n",
            "INFO:__main__:  Batch 10/66: loss=2.5806\n",
            "INFO:__main__:  Batch 20/66: loss=2.6174\n",
            "INFO:__main__:  Batch 30/66: loss=2.7409\n",
            "INFO:__main__:  Batch 40/66: loss=2.6016\n",
            "INFO:__main__:  Batch 50/66: loss=2.7855\n",
            "INFO:__main__:  Batch 60/66: loss=2.7581\n",
            "INFO:__main__:Train loss: 2.6709\n",
            "INFO:__main__:Val loss: 2.5580\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.5580)\n",
            "INFO:__main__:\n",
            "Epoch 9/50\n",
            "INFO:__main__:  Batch 10/66: loss=2.7857\n",
            "INFO:__main__:  Batch 20/66: loss=2.5802\n",
            "INFO:__main__:  Batch 30/66: loss=2.5984\n",
            "INFO:__main__:  Batch 40/66: loss=2.6253\n",
            "INFO:__main__:  Batch 50/66: loss=2.7248\n",
            "INFO:__main__:  Batch 60/66: loss=2.4515\n",
            "INFO:__main__:Train loss: 2.6040\n",
            "INFO:__main__:Val loss: 2.4900\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.4900)\n",
            "INFO:__main__:\n",
            "Epoch 10/50\n",
            "INFO:__main__:  Batch 10/66: loss=2.5713\n",
            "INFO:__main__:  Batch 20/66: loss=2.5596\n",
            "INFO:__main__:  Batch 30/66: loss=2.4804\n",
            "INFO:__main__:  Batch 40/66: loss=2.5675\n",
            "INFO:__main__:  Batch 50/66: loss=2.3974\n",
            "INFO:__main__:  Batch 60/66: loss=2.3873\n",
            "INFO:__main__:Train loss: 2.5329\n",
            "INFO:__main__:Val loss: 2.4254\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.4254)\n",
            "INFO:__main__:\n",
            "Epoch 11/50\n",
            "INFO:__main__:  Batch 10/66: loss=2.4348\n",
            "INFO:__main__:  Batch 20/66: loss=2.4306\n",
            "INFO:__main__:  Batch 30/66: loss=2.4310\n",
            "INFO:__main__:  Batch 40/66: loss=2.5431\n",
            "INFO:__main__:  Batch 50/66: loss=2.3329\n",
            "INFO:__main__:  Batch 60/66: loss=2.4267\n",
            "INFO:__main__:Train loss: 2.4688\n",
            "INFO:__main__:Val loss: 2.3637\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.3637)\n",
            "INFO:__main__:\n",
            "Epoch 12/50\n",
            "INFO:__main__:  Batch 10/66: loss=2.3508\n",
            "INFO:__main__:  Batch 20/66: loss=2.5388\n",
            "INFO:__main__:  Batch 30/66: loss=2.3341\n",
            "INFO:__main__:  Batch 40/66: loss=2.4041\n",
            "INFO:__main__:  Batch 50/66: loss=2.3476\n",
            "INFO:__main__:  Batch 60/66: loss=2.5126\n",
            "INFO:__main__:Train loss: 2.4103\n",
            "INFO:__main__:Val loss: 2.3100\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.3100)\n",
            "INFO:__main__:\n",
            "Epoch 13/50\n",
            "INFO:__main__:  Batch 10/66: loss=2.3202\n",
            "INFO:__main__:  Batch 20/66: loss=2.3357\n",
            "INFO:__main__:  Batch 30/66: loss=2.4145\n",
            "INFO:__main__:  Batch 40/66: loss=2.3860\n",
            "INFO:__main__:  Batch 50/66: loss=2.3461\n",
            "INFO:__main__:  Batch 60/66: loss=2.2844\n",
            "INFO:__main__:Train loss: 2.3545\n",
            "INFO:__main__:Val loss: 2.2609\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.2609)\n",
            "INFO:__main__:\n",
            "Epoch 14/50\n",
            "INFO:__main__:  Batch 10/66: loss=2.3438\n",
            "INFO:__main__:  Batch 20/66: loss=2.2735\n",
            "INFO:__main__:  Batch 30/66: loss=2.2443\n",
            "INFO:__main__:  Batch 40/66: loss=2.2785\n",
            "INFO:__main__:  Batch 50/66: loss=2.2349\n",
            "INFO:__main__:  Batch 60/66: loss=2.1969\n",
            "INFO:__main__:Train loss: 2.3041\n",
            "INFO:__main__:Val loss: 2.2098\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.2098)\n",
            "INFO:__main__:\n",
            "Epoch 15/50\n",
            "INFO:__main__:  Batch 10/66: loss=2.2013\n",
            "INFO:__main__:  Batch 20/66: loss=2.1934\n",
            "INFO:__main__:  Batch 30/66: loss=2.3572\n",
            "INFO:__main__:  Batch 40/66: loss=2.3432\n",
            "INFO:__main__:  Batch 50/66: loss=2.2456\n",
            "INFO:__main__:  Batch 60/66: loss=2.1831\n",
            "INFO:__main__:Train loss: 2.2546\n",
            "INFO:__main__:Val loss: 2.1686\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.1686)\n",
            "INFO:__main__:\n",
            "Epoch 16/50\n",
            "INFO:__main__:  Batch 10/66: loss=2.2997\n",
            "INFO:__main__:  Batch 20/66: loss=2.2282\n",
            "INFO:__main__:  Batch 30/66: loss=2.1407\n",
            "INFO:__main__:  Batch 40/66: loss=2.1896\n",
            "INFO:__main__:  Batch 50/66: loss=2.1702\n",
            "INFO:__main__:  Batch 60/66: loss=2.0774\n",
            "INFO:__main__:Train loss: 2.2070\n",
            "INFO:__main__:Val loss: 2.1247\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.1247)\n",
            "INFO:__main__:\n",
            "Epoch 17/50\n",
            "INFO:__main__:  Batch 10/66: loss=2.2039\n",
            "INFO:__main__:  Batch 20/66: loss=2.1712\n",
            "INFO:__main__:  Batch 30/66: loss=2.1478\n",
            "INFO:__main__:  Batch 40/66: loss=2.0617\n",
            "INFO:__main__:  Batch 50/66: loss=2.2664\n",
            "INFO:__main__:  Batch 60/66: loss=2.1613\n",
            "INFO:__main__:Train loss: 2.1650\n",
            "INFO:__main__:Val loss: 2.0880\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.0880)\n",
            "INFO:__main__:\n",
            "Epoch 18/50\n",
            "INFO:__main__:  Batch 10/66: loss=2.0663\n",
            "INFO:__main__:  Batch 20/66: loss=2.2045\n",
            "INFO:__main__:  Batch 30/66: loss=2.0923\n",
            "INFO:__main__:  Batch 40/66: loss=2.1481\n",
            "INFO:__main__:  Batch 50/66: loss=2.2258\n",
            "INFO:__main__:  Batch 60/66: loss=2.0508\n",
            "INFO:__main__:Train loss: 2.1214\n",
            "INFO:__main__:Val loss: 2.0558\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.0558)\n",
            "INFO:__main__:\n",
            "Epoch 19/50\n",
            "INFO:__main__:  Batch 10/66: loss=2.0328\n",
            "INFO:__main__:  Batch 20/66: loss=2.0018\n",
            "INFO:__main__:  Batch 30/66: loss=2.0347\n",
            "INFO:__main__:  Batch 40/66: loss=2.0836\n",
            "INFO:__main__:  Batch 50/66: loss=2.0418\n",
            "INFO:__main__:  Batch 60/66: loss=2.1469\n",
            "INFO:__main__:Train loss: 2.0839\n",
            "INFO:__main__:Val loss: 2.0193\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=2.0193)\n",
            "INFO:__main__:\n",
            "Epoch 20/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.9888\n",
            "INFO:__main__:  Batch 20/66: loss=2.0063\n",
            "INFO:__main__:  Batch 30/66: loss=2.0468\n",
            "INFO:__main__:  Batch 40/66: loss=2.1326\n",
            "INFO:__main__:  Batch 50/66: loss=2.1003\n",
            "INFO:__main__:  Batch 60/66: loss=2.0189\n",
            "INFO:__main__:Train loss: 2.0482\n",
            "INFO:__main__:Val loss: 1.9882\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.9882)\n",
            "INFO:__main__:\n",
            "Epoch 21/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.9631\n",
            "INFO:__main__:  Batch 20/66: loss=2.0658\n",
            "INFO:__main__:  Batch 30/66: loss=1.9584\n",
            "INFO:__main__:  Batch 40/66: loss=2.1274\n",
            "INFO:__main__:  Batch 50/66: loss=2.0764\n",
            "INFO:__main__:  Batch 60/66: loss=2.0876\n",
            "INFO:__main__:Train loss: 2.0174\n",
            "INFO:__main__:Val loss: 1.9619\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.9619)\n",
            "INFO:__main__:\n",
            "Epoch 22/50\n",
            "INFO:__main__:  Batch 10/66: loss=2.0200\n",
            "INFO:__main__:  Batch 20/66: loss=1.9531\n",
            "INFO:__main__:  Batch 30/66: loss=1.9391\n",
            "INFO:__main__:  Batch 40/66: loss=1.9746\n",
            "INFO:__main__:  Batch 50/66: loss=2.0187\n",
            "INFO:__main__:  Batch 60/66: loss=2.0637\n",
            "INFO:__main__:Train loss: 1.9880\n",
            "INFO:__main__:Val loss: 1.9399\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.9399)\n",
            "INFO:__main__:\n",
            "Epoch 23/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.9524\n",
            "INFO:__main__:  Batch 20/66: loss=1.9584\n",
            "INFO:__main__:  Batch 30/66: loss=1.9915\n",
            "INFO:__main__:  Batch 40/66: loss=1.8452\n",
            "INFO:__main__:  Batch 50/66: loss=1.9666\n",
            "INFO:__main__:  Batch 60/66: loss=1.9735\n",
            "INFO:__main__:Train loss: 1.9623\n",
            "INFO:__main__:Val loss: 1.9145\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.9145)\n",
            "INFO:__main__:\n",
            "Epoch 24/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.9895\n",
            "INFO:__main__:  Batch 20/66: loss=2.0454\n",
            "INFO:__main__:  Batch 30/66: loss=1.9284\n",
            "INFO:__main__:  Batch 40/66: loss=1.9143\n",
            "INFO:__main__:  Batch 50/66: loss=2.0121\n",
            "INFO:__main__:  Batch 60/66: loss=1.9548\n",
            "INFO:__main__:Train loss: 1.9341\n",
            "INFO:__main__:Val loss: 1.8944\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.8944)\n",
            "INFO:__main__:\n",
            "Epoch 25/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.8785\n",
            "INFO:__main__:  Batch 20/66: loss=1.8952\n",
            "INFO:__main__:  Batch 30/66: loss=1.8803\n",
            "INFO:__main__:  Batch 40/66: loss=1.8915\n",
            "INFO:__main__:  Batch 50/66: loss=1.9976\n",
            "INFO:__main__:  Batch 60/66: loss=1.8342\n",
            "INFO:__main__:Train loss: 1.9087\n",
            "INFO:__main__:Val loss: 1.8790\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.8790)\n",
            "INFO:__main__:\n",
            "Epoch 26/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.8789\n",
            "INFO:__main__:  Batch 20/66: loss=1.9297\n",
            "INFO:__main__:  Batch 30/66: loss=1.9360\n",
            "INFO:__main__:  Batch 40/66: loss=1.8850\n",
            "INFO:__main__:  Batch 50/66: loss=1.9494\n",
            "INFO:__main__:  Batch 60/66: loss=2.0007\n",
            "INFO:__main__:Train loss: 1.8913\n",
            "INFO:__main__:Val loss: 1.8599\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.8599)\n",
            "INFO:__main__:\n",
            "Epoch 27/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.8577\n",
            "INFO:__main__:  Batch 20/66: loss=1.9452\n",
            "INFO:__main__:  Batch 30/66: loss=1.8800\n",
            "INFO:__main__:  Batch 40/66: loss=1.8780\n",
            "INFO:__main__:  Batch 50/66: loss=1.8692\n",
            "INFO:__main__:  Batch 60/66: loss=1.9917\n",
            "INFO:__main__:Train loss: 1.8720\n",
            "INFO:__main__:Val loss: 1.8449\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.8449)\n",
            "INFO:__main__:\n",
            "Epoch 28/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.9411\n",
            "INFO:__main__:  Batch 20/66: loss=1.8115\n",
            "INFO:__main__:  Batch 30/66: loss=1.8053\n",
            "INFO:__main__:  Batch 40/66: loss=1.7773\n",
            "INFO:__main__:  Batch 50/66: loss=1.8805\n",
            "INFO:__main__:  Batch 60/66: loss=1.8430\n",
            "INFO:__main__:Train loss: 1.8556\n",
            "INFO:__main__:Val loss: 1.8325\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.8325)\n",
            "INFO:__main__:\n",
            "Epoch 29/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.8121\n",
            "INFO:__main__:  Batch 20/66: loss=1.8373\n",
            "INFO:__main__:  Batch 30/66: loss=1.8713\n",
            "INFO:__main__:  Batch 40/66: loss=1.8127\n",
            "INFO:__main__:  Batch 50/66: loss=1.9144\n",
            "INFO:__main__:  Batch 60/66: loss=1.8616\n",
            "INFO:__main__:Train loss: 1.8380\n",
            "INFO:__main__:Val loss: 1.8203\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.8203)\n",
            "INFO:__main__:\n",
            "Epoch 30/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.8100\n",
            "INFO:__main__:  Batch 20/66: loss=1.8285\n",
            "INFO:__main__:  Batch 30/66: loss=1.7989\n",
            "INFO:__main__:  Batch 40/66: loss=1.7900\n",
            "INFO:__main__:  Batch 50/66: loss=1.8753\n",
            "INFO:__main__:  Batch 60/66: loss=1.8249\n",
            "INFO:__main__:Train loss: 1.8201\n",
            "INFO:__main__:Val loss: 1.8079\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.8079)\n",
            "INFO:__main__:\n",
            "Epoch 31/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.8661\n",
            "INFO:__main__:  Batch 20/66: loss=1.8574\n",
            "INFO:__main__:  Batch 30/66: loss=1.8434\n",
            "INFO:__main__:  Batch 40/66: loss=1.8306\n",
            "INFO:__main__:  Batch 50/66: loss=1.7841\n",
            "INFO:__main__:  Batch 60/66: loss=1.7542\n",
            "INFO:__main__:Train loss: 1.8112\n",
            "INFO:__main__:Val loss: 1.8007\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.8007)\n",
            "INFO:__main__:\n",
            "Epoch 32/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.8638\n",
            "INFO:__main__:  Batch 20/66: loss=1.7644\n",
            "INFO:__main__:  Batch 30/66: loss=1.7706\n",
            "INFO:__main__:  Batch 40/66: loss=1.8162\n",
            "INFO:__main__:  Batch 50/66: loss=1.8091\n",
            "INFO:__main__:  Batch 60/66: loss=1.7666\n",
            "INFO:__main__:Train loss: 1.7964\n",
            "INFO:__main__:Val loss: 1.7897\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7897)\n",
            "INFO:__main__:\n",
            "Epoch 33/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.7083\n",
            "INFO:__main__:  Batch 20/66: loss=1.8863\n",
            "INFO:__main__:  Batch 30/66: loss=1.8196\n",
            "INFO:__main__:  Batch 40/66: loss=1.8044\n",
            "INFO:__main__:  Batch 50/66: loss=1.8403\n",
            "INFO:__main__:  Batch 60/66: loss=1.7900\n",
            "INFO:__main__:Train loss: 1.7882\n",
            "INFO:__main__:Val loss: 1.7835\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7835)\n",
            "INFO:__main__:\n",
            "Epoch 34/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.7900\n",
            "INFO:__main__:  Batch 20/66: loss=1.7899\n",
            "INFO:__main__:  Batch 30/66: loss=1.7659\n",
            "INFO:__main__:  Batch 40/66: loss=1.7903\n",
            "INFO:__main__:  Batch 50/66: loss=1.7403\n",
            "INFO:__main__:  Batch 60/66: loss=1.7797\n",
            "INFO:__main__:Train loss: 1.7777\n",
            "INFO:__main__:Val loss: 1.7752\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7752)\n",
            "INFO:__main__:\n",
            "Epoch 35/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.7380\n",
            "INFO:__main__:  Batch 20/66: loss=1.7677\n",
            "INFO:__main__:  Batch 30/66: loss=1.7494\n",
            "INFO:__main__:  Batch 40/66: loss=1.7558\n",
            "INFO:__main__:  Batch 50/66: loss=1.7800\n",
            "INFO:__main__:  Batch 60/66: loss=1.7683\n",
            "INFO:__main__:Train loss: 1.7676\n",
            "INFO:__main__:Val loss: 1.7687\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7687)\n",
            "INFO:__main__:\n",
            "Epoch 36/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.7079\n",
            "INFO:__main__:  Batch 20/66: loss=1.7193\n",
            "INFO:__main__:  Batch 30/66: loss=1.7159\n",
            "INFO:__main__:  Batch 40/66: loss=1.7592\n",
            "INFO:__main__:  Batch 50/66: loss=1.7304\n",
            "INFO:__main__:  Batch 60/66: loss=1.7999\n",
            "INFO:__main__:Train loss: 1.7617\n",
            "INFO:__main__:Val loss: 1.7639\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7639)\n",
            "INFO:__main__:\n",
            "Epoch 37/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.7336\n",
            "INFO:__main__:  Batch 20/66: loss=1.7183\n",
            "INFO:__main__:  Batch 30/66: loss=1.7848\n",
            "INFO:__main__:  Batch 40/66: loss=1.6953\n",
            "INFO:__main__:  Batch 50/66: loss=1.7476\n",
            "INFO:__main__:  Batch 60/66: loss=1.7144\n",
            "INFO:__main__:Train loss: 1.7534\n",
            "INFO:__main__:Val loss: 1.7589\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7589)\n",
            "INFO:__main__:\n",
            "Epoch 38/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.8352\n",
            "INFO:__main__:  Batch 20/66: loss=1.7193\n",
            "INFO:__main__:  Batch 30/66: loss=1.7385\n",
            "INFO:__main__:  Batch 40/66: loss=1.7166\n",
            "INFO:__main__:  Batch 50/66: loss=1.7806\n",
            "INFO:__main__:  Batch 60/66: loss=1.7529\n",
            "INFO:__main__:Train loss: 1.7492\n",
            "INFO:__main__:Val loss: 1.7556\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7556)\n",
            "INFO:__main__:\n",
            "Epoch 39/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.8071\n",
            "INFO:__main__:  Batch 20/66: loss=1.7442\n",
            "INFO:__main__:  Batch 30/66: loss=1.7598\n",
            "INFO:__main__:  Batch 40/66: loss=1.8225\n",
            "INFO:__main__:  Batch 50/66: loss=1.7324\n",
            "INFO:__main__:  Batch 60/66: loss=1.7347\n",
            "INFO:__main__:Train loss: 1.7449\n",
            "INFO:__main__:Val loss: 1.7531\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7531)\n",
            "INFO:__main__:\n",
            "Epoch 40/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.8349\n",
            "INFO:__main__:  Batch 20/66: loss=1.6942\n",
            "INFO:__main__:  Batch 30/66: loss=1.6834\n",
            "INFO:__main__:  Batch 40/66: loss=1.7101\n",
            "INFO:__main__:  Batch 50/66: loss=1.6924\n",
            "INFO:__main__:  Batch 60/66: loss=1.8681\n",
            "INFO:__main__:Train loss: 1.7363\n",
            "INFO:__main__:Val loss: 1.7495\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7495)\n",
            "INFO:__main__:\n",
            "Epoch 41/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.6699\n",
            "INFO:__main__:  Batch 20/66: loss=1.7095\n",
            "INFO:__main__:  Batch 30/66: loss=1.7738\n",
            "INFO:__main__:  Batch 40/66: loss=1.7739\n",
            "INFO:__main__:  Batch 50/66: loss=1.8018\n",
            "INFO:__main__:  Batch 60/66: loss=1.8264\n",
            "INFO:__main__:Train loss: 1.7354\n",
            "INFO:__main__:Val loss: 1.7489\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7489)\n",
            "INFO:__main__:\n",
            "Epoch 42/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.6995\n",
            "INFO:__main__:  Batch 20/66: loss=1.7389\n",
            "INFO:__main__:  Batch 30/66: loss=1.7406\n",
            "INFO:__main__:  Batch 40/66: loss=1.6324\n",
            "INFO:__main__:  Batch 50/66: loss=1.7508\n",
            "INFO:__main__:  Batch 60/66: loss=1.7955\n",
            "INFO:__main__:Train loss: 1.7306\n",
            "INFO:__main__:Val loss: 1.7467\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7467)\n",
            "INFO:__main__:\n",
            "Epoch 43/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.7517\n",
            "INFO:__main__:  Batch 20/66: loss=1.6811\n",
            "INFO:__main__:  Batch 30/66: loss=1.7293\n",
            "INFO:__main__:  Batch 40/66: loss=1.7548\n",
            "INFO:__main__:  Batch 50/66: loss=1.6886\n",
            "INFO:__main__:  Batch 60/66: loss=1.7790\n",
            "INFO:__main__:Train loss: 1.7302\n",
            "INFO:__main__:Val loss: 1.7447\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7447)\n",
            "INFO:__main__:\n",
            "Epoch 44/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.7086\n",
            "INFO:__main__:  Batch 20/66: loss=1.6752\n",
            "INFO:__main__:  Batch 30/66: loss=1.7221\n",
            "INFO:__main__:  Batch 40/66: loss=1.7260\n",
            "INFO:__main__:  Batch 50/66: loss=1.7203\n",
            "INFO:__main__:  Batch 60/66: loss=1.7024\n",
            "INFO:__main__:Train loss: 1.7279\n",
            "INFO:__main__:Val loss: 1.7434\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7434)\n",
            "INFO:__main__:\n",
            "Epoch 45/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.7023\n",
            "INFO:__main__:  Batch 20/66: loss=1.6949\n",
            "INFO:__main__:  Batch 30/66: loss=1.6836\n",
            "INFO:__main__:  Batch 40/66: loss=1.7248\n",
            "INFO:__main__:  Batch 50/66: loss=1.6553\n",
            "INFO:__main__:  Batch 60/66: loss=1.7067\n",
            "INFO:__main__:Train loss: 1.7267\n",
            "INFO:__main__:Val loss: 1.7426\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7426)\n",
            "INFO:__main__:\n",
            "Epoch 46/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.6824\n",
            "INFO:__main__:  Batch 20/66: loss=1.7281\n",
            "INFO:__main__:  Batch 30/66: loss=1.7535\n",
            "INFO:__main__:  Batch 40/66: loss=1.6804\n",
            "INFO:__main__:  Batch 50/66: loss=1.6515\n",
            "INFO:__main__:  Batch 60/66: loss=1.7412\n",
            "INFO:__main__:Train loss: 1.7242\n",
            "INFO:__main__:Val loss: 1.7420\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7420)\n",
            "INFO:__main__:\n",
            "Epoch 47/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.7402\n",
            "INFO:__main__:  Batch 20/66: loss=1.7472\n",
            "INFO:__main__:  Batch 30/66: loss=1.7573\n",
            "INFO:__main__:  Batch 40/66: loss=1.7379\n",
            "INFO:__main__:  Batch 50/66: loss=1.6641\n",
            "INFO:__main__:  Batch 60/66: loss=1.6947\n",
            "INFO:__main__:Train loss: 1.7228\n",
            "INFO:__main__:Val loss: 1.7416\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7416)\n",
            "INFO:__main__:\n",
            "Epoch 48/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.7464\n",
            "INFO:__main__:  Batch 20/66: loss=1.7386\n",
            "INFO:__main__:  Batch 30/66: loss=1.7223\n",
            "INFO:__main__:  Batch 40/66: loss=1.7468\n",
            "INFO:__main__:  Batch 50/66: loss=1.6636\n",
            "INFO:__main__:  Batch 60/66: loss=1.6670\n",
            "INFO:__main__:Train loss: 1.7205\n",
            "INFO:__main__:Val loss: 1.7415\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7415)\n",
            "INFO:__main__:\n",
            "Epoch 49/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.7748\n",
            "INFO:__main__:  Batch 20/66: loss=1.7626\n",
            "INFO:__main__:  Batch 30/66: loss=1.7397\n",
            "INFO:__main__:  Batch 40/66: loss=1.6804\n",
            "INFO:__main__:  Batch 50/66: loss=1.7225\n",
            "INFO:__main__:  Batch 60/66: loss=1.7083\n",
            "INFO:__main__:Train loss: 1.7229\n",
            "INFO:__main__:Val loss: 1.7413\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7413)\n",
            "INFO:__main__:\n",
            "Epoch 50/50\n",
            "INFO:__main__:  Batch 10/66: loss=1.7040\n",
            "INFO:__main__:  Batch 20/66: loss=1.6698\n",
            "INFO:__main__:  Batch 30/66: loss=1.6784\n",
            "INFO:__main__:  Batch 40/66: loss=1.6900\n",
            "INFO:__main__:  Batch 50/66: loss=1.6698\n",
            "INFO:__main__:  Batch 60/66: loss=1.6390\n",
            "INFO:__main__:Train loss: 1.7238\n",
            "INFO:__main__:Val loss: 1.7413\n",
            "INFO:__main__:✓ Saving checkpoint (val_loss=1.7413)\n",
            "INFO:__main__:\n",
            "✓ Training complete\n",
            "INFO:__main__:Best model saved to: ../models/transformer_v42.pt\n"
          ]
        }
      ],
      "source": [
        "!python train.py --data training/data/processed/train.npz --output ../models/transformer_v42.pt --device cuda"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
